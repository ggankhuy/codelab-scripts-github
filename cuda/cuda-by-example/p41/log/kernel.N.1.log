==PROF== Connected to process 2826 (/root/extdir/gg/git/codelab/gpu/cuda/cuda-by-example/p41/a.out)
==PROF== Profiling "add(int *, int *, int *)" - 0: 0%....50%....100% - 8 passes
==PROF== Disconnected from process 2826
0: 0 + 200 = 200
1000: 1000 + 1200 = 2200
2000: 2000 + 2200 = 4200
3000: 3000 + 3200 = 6200
4000: 4000 + 4200 = 8200
5000: 5000 + 5200 = 10200
6000: 6000 + 6200 = 12200
7000: 7000 + 7200 = 14200
8000: 8000 + 8200 = 16200
[2826] a.out@127.0.0.1
  add(int *, int *, int *) (8192, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         6.84
    SM Frequency            cycle/nsecond         1.61
    Elapsed Cycles                  cycle        18140
    Memory Throughput                   %         6.80
    DRAM Throughput                     %         1.41
    Duration                      usecond        11.23
    L1/TEX Cache Throughput             %        10.65
    L2 Cache Throughput                 %         3.83
    SM Active Cycles                cycle     11533.20
    Compute (SM) Throughput             %         6.80
    ----------------------- ------------- ------------

    OPT   This kernel exhibits low compute throughput and memory bandwidth utilization relative to the peak performance 
          of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak typically indicate    
          latency issues. Look at Scheduler Statistics and Warp State Statistics for potential reasons.                 

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                     1
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   8192
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            8192
    Waves Per SM                                               12.80
    -------------------------------- --------------- ---------------

    OPT   Estimated Speedup: 3.03%                                                                                      
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block          128
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block           32
    Theoretical Active Warps per SM        warp           16
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %        30.48
    Achieved Active Warps Per SM           warp         9.75
    ------------------------------- ----------- ------------

    OPT   Estimated Speedup: 39.03%                                                                                     
          This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM. This   
          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory. The difference     
          between calculated theoretical (50.0%) and measured achieved occupancy (30.5%) can be the result of warp      
          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    
          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           
          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           
          optimizing occupancy.                                                                                         

