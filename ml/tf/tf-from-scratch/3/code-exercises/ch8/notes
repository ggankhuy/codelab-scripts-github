
RNN CELL:

t[h] = Whh * t[h-1] + b[hh] -> 
output of hidden state/transformer hidden state = 
weight of hidden layer * prev hidden state + bias of hidden layer. 

t[x] = Wih * x[t] + b[ih] ->
output of input layer/transformed input state = 
weigth of input layer * input data + bias of input layer.

h[t] = tanh ( t[x] + t[h] ) ->
output of current iteration/updated hidden state = 
activation layer ( hidden state output + input layer output)

5.11.2024
decided to skip bidir rnn completion. may visit later.
deciding to skip gru cell p149

p134 train function calls. 

sbs_rnn=StepByStep(model, loss, optimizer)
sbs_rnn.set_loaders(train_loader, test_loader)
sbs_rnn.train(100)
    for epoch in range(n_epochs):
            loss = self._mini_batch(validation=False)
                #inside mini_batch:
                for i, (x_batch, y_batch) in enumerate(data_loader):
                    mini_batch_loss = step_fn(x_batch, y_batch)
                        step_fn--->self.train_step_fn()-->_make_train_step_fn-->perform_train_step_fn()
                            #inside perform_train_step_fn
                            self.model.train()


            with torch.no_grad():
                val_loss = self._mini_batch(validation=True)
                    #inside mini_batch:
                    for i, (x_batch, y_batch) in enumerate(data_loader):
                        mini_batch_loss = step_fn(x_batch, y_batch)
                            step_fn--->self.val_step_fn()->_make_val_step_fn-->perform_val_step_fn()
                                #inside perform_val_step_fn
                                self.model.eval()

loader_apply(test_loader, sbs_rnn.correct) where test_loader=[128,[[4,2],[1]]]
    for each batch in test_loader, call correct(x,y) where x=[16,4,2] and y=[16] therefore iteratest 128/16=8 times.    
    y=hat compute => [16,1]=[16]
    compute predicted => [16,1]
    returns



    





